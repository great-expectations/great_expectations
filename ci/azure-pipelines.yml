# This file is responsible for configuring the `great_expectations` pipeline (https://dev.azure.com/great-expectations/great_expectations/_build)
#
# The pipeline is run under the following conditions:
#   - On the develop branch when a weekly release is being cut
#   - On the develop branch as scheduled by the below cron job
#
#  `great_expectations` runs the entire test suite and is meant to safeguard the codebase against any errors that may have slipped through the cracks
#  from `dependency_graph`. Additionally, it is responsible for deployment during the weekly release cut.

schedules:
- cron: 0 */3 * * *
  displayName: Scheduled Runs
  branches:
    include:
    - develop
  always: false # Will only trigger if the state of the codebase has changed since the last scheduled run

trigger:
  tags:
    include:
      - '*' # Should be SemVer for a successful release but any tag will trigger the build

resources:
  containers:
  - container: postgres
    image: postgres:11
    ports:
    - 5432:5432
    env:
      POSTGRES_DB: "test_ci"
      POSTGRES_HOST_AUTH_METHOD: "trust"
  - container: mysql
    image: mysql:8.0.20
    ports:
      - 3306:3306
    env:
      MYSQL_ALLOW_EMPTY_PASSWORD: "yes"
      MYSQL_DATABASE: test_ci
  - container: mssql
    image: mcr.microsoft.com/mssql/server:2019-latest
    env:
      ACCEPT_EULA: Y
      MSSQL_SA_PASSWORD: ReallyStrongPwd1234%^&*
      MSSQL_DB: test_ci
      MSSQL_PID: Developer
    ports:
      - 1433:1433
  - container: trino
    image: trinodb/trino:400
    ports:
      - 8088:8080

# The pipeline is run under two primary conditions: if cutting a release or as scheduled by the above cron job.
variables:
  isReleasePrep: $[contains(variables['Build.SourceBranch'], 'release')]
  isRelease: $[startsWith(variables['Build.SourceBranch'], 'refs/tags/')]
  isScheduled: $[and(eq(variables['Build.SourceBranch'], 'refs/heads/develop'), eq(variables['Build.Reason'], 'Schedule'))]
  isManual: $[eq(variables['Build.Reason'], 'Manual')]

stages:
  - stage: lint
    pool:
      vmImage: 'ubuntu-latest'

    jobs:
      - job: lint
        condition: or(eq(variables.isScheduled, true), eq(variables.isReleasePrep, true), eq(variables.isRelease, true), eq(variables.isManual, true))
        steps:
          - task: UsePythonVersion@0
            inputs:
              versionSpec: 3.8
            displayName: 'Use Python 3.8'

          - script: |
              pip install $(grep -E '^(black|invoke|ruff)' reqs/requirements-dev-contrib.txt)
              EXIT_STATUS=0
              invoke fmt --check || EXIT_STATUS=$?
              invoke lint || EXIT_STATUS=$?
              exit $EXIT_STATUS

  - stage: docker_tests
    dependsOn: [lint]
    condition: or(eq(variables.isScheduled, true), eq(variables.isReleasePrep, true), eq(variables.isRelease, true), eq(variables.isManual, true))
    pool:
      vmImage: 'ubuntu-latest'
    jobs:
      - job: make_suffix
        steps:
          - bash: |
              # Create a docker allowed image name from the branch name
              eval `python - <<EOF
              import os
              import re
              branch_name = os.environ["BRANCH_NAME"]
              new_name = re.sub('[^a-zA-Z0-9_\-\.]', '-', branch_name)
              print(f'export "IMAGE_BRANCH"="{new_name}"')
              EOF`

              EPOCH=`date +%s`
              SUFFIX="${IMAGE_BRANCH}_${EPOCH}_${RANDOM}"
              echo "Generated image suffix: ${SUFFIX}"
              echo "##vso[task.setvariable variable=IMAGE_SUFFIX;isOutput=true]${SUFFIX}"
            name: suffix
            env:
              BRANCH_NAME: $(Build.SourceBranchName)

      - job: build_push
        dependsOn: make_suffix
        condition: succeeded()
        strategy:
          matrix:
            Python38:
              python.version: '3.8'
            Python39:
              python.version: '3.9'
            Python310:
              python.version: '3.10'
            Python311:
              python.version: '3.11'
        variables:
          IMAGE_SUFFIX: $[ dependencies.make_suffix.outputs['suffix.IMAGE_SUFFIX'] ]
          GE_USAGE_STATISTICS_URL: "https://qa.stats.greatexpectations.io/great_expectations/v1/usage_statistics"
        steps:
          - bash: |
              DOCKER_TAG="$(python.version)_$(IMAGE_SUFFIX)"
              # Remove /refs/heads or /refs/tags from the FULL_BRANCH_NAME
              BRANCH_NAME="${FULL_BRANCH_NAME/#refs\/heads\/}"
              BRANCH_NAME="${BRANCH_NAME/#refs\/tags\/}"
              # Use the local branch that is already present, but prune it's depth since we don't need the git history.
              CMD="docker buildx build -f docker/Dockerfile.tests --tag greatexpectations/test:${DOCKER_TAG} "
              CMD+="--target test --build-arg PYTHON_VERSION=$(python.version) "
              CMD+="--build-arg GE_USAGE_STATISTICS_URL=\"$(GE_USAGE_STATISTICS_URL)\" "
              CMD+="--build-arg SOURCE=github --build-arg BRANCH=\"${BRANCH_NAME}\" ."
              echo ${CMD}
              eval ${CMD}
            displayName: 'Build python $(python.version) image'
            env:
              FULL_BRANCH_NAME: $(Build.SourceBranch)

          - bash: |
              docker login -u ${DOCKER_USER} -p ${DOCKER_TOKEN}
              DOCKER_TAG="$(python.version)_$(IMAGE_SUFFIX)"
              PUSH="docker push greatexpectations/test:${DOCKER_TAG}"
              echo ${PUSH}
              eval ${PUSH}
              echo "Docker image can be found at https://hub.docker.com/r/greatexpectations/test/tags"
              echo "Pull it locally using: docker pull greatexpectations/test:${DOCKER_TAG}"
            displayName: 'Push python $(python.version) image'
            env:
              DOCKER_USER: $(DOCKER_USER)
              DOCKER_TOKEN: $(DOCKER_TOKEN)

      - job: test_majority
        dependsOn: [make_suffix, build_push]
        condition: succeeded()
        timeoutInMinutes: 120
        strategy:
          matrix:
            Python38:
              python.version: '3.8'
            Python39:
              python.version: '3.9'
            Python310:
              python.version: '3.10'
            Python311:
              python.version: '3.11'
        services:
          postgres: postgres
        variables:
          IMAGE_SUFFIX: $[ dependencies.make_suffix.outputs['suffix.IMAGE_SUFFIX'] ]
        steps:
          - bash: |
              DOCKER_TAG="$(python.version)_$(IMAGE_SUFFIX)"
              mkdir coverage
              PYTEST="docker run -e AWS_ACCESS_KEY_ID=$(AWS_ACCESS_KEY_ID) "
              PYTEST+="-e AWS_SECRET_ACCESS_KEY=$(AWS_SECRET_ACCESS_KEY) -e AWS_DEFAULT_REGION=$(AWS_DEFAULT_REGION) "
              PYTEST+="--network=host --mount type=bind,source=${PWD}/coverage,target=/coverage "
              # We don't run e2e because we don't want to test connections to external dependencies here.
              PYTEST+="greatexpectations/test:${DOCKER_TAG} /bin/bash -c \"pytest -m 'not e2e' "
              PYTEST+="--random-order --postgresql --cloud "
              # We should be able to specify --cov-report=xml:path but this doesn't work so we write to the default
              # path and then mv it.
              PYTEST+="--junitxml=/coverage/junit/test-results.xml --cov=. --cov-report=xml --cov-report=html:/coverage/htmlcov "
              PYTEST+="--ignore=tests/cli --ignore=tests/integration/usage_statistics "
              # The onboarding data assistant tests do not succeed in random order and are run on their own below.
              PYTEST+="--ignore=tests/rule_based_profiler/data_assistant/test_onboarding_data_assistant.py "
              # The last \" closes the quote from bash -c \"
              PYTEST+="&& mv coverage.xml /coverage/coverage.xml\""

              echo ${PYTEST}
              eval ${PYTEST}
            displayName: 'Pytest $(python.version)'
            env:
              AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
              AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

          - task: PublishTestResults@2
            condition: succeededOrFailed()
            inputs:
              testResultsFiles: '$(System.DefaultWorkingDirectory)/coverage/**/test-*.xml'
              testRunTitle: 'Publish test results for Python $(python.version)'

          - task: PublishCodeCoverageResults@1
            inputs:
              codeCoverageTool: Cobertura
              summaryFileLocation: '$(System.DefaultWorkingDirectory)/coverage/coverage.xml'
              reportDirectory: '$(System.DefaultWorkingDirectory)/coverage/htmlcov'

      - job: test_onboarding_data_assistant
        dependsOn: [ make_suffix, build_push ]
        condition: succeeded()
        strategy:
          matrix:
            Python38:
              python.version: '3.8'
            Python39:
              python.version: '3.9'
            Python310:
              python.version: '3.10'
        variables:
          IMAGE_SUFFIX: $[ dependencies.make_suffix.outputs['suffix.IMAGE_SUFFIX'] ]
        steps:
          - bash: |
              DOCKER_TAG="$(python.version)_$(IMAGE_SUFFIX)"
              PYTEST="docker run greatexpectations/test:${DOCKER_TAG} "
              # These tests currently do no succeed in random order.
              PYTEST+="pytest tests/rule_based_profiler/data_assistant/test_onboarding_data_assistant.py"
              echo ${PYTEST}
              eval ${PYTEST}
            displayName: 'Pytest Onboarding Data Assistant'

      - job: test_spark
        dependsOn: [ make_suffix, build_push ]
        condition: succeeded()
        timeoutInMinutes: 120
        strategy:
          matrix:
            Python38:
              python.version: '3.8'
            Python39:
              python.version: '3.9'
            Python310:
              python.version: '3.10'
        variables:
          IMAGE_SUFFIX: $[ dependencies.make_suffix.outputs['suffix.IMAGE_SUFFIX'] ]
        steps:
          - bash: |
              DOCKER_TAG="$(python.version)_$(IMAGE_SUFFIX)"
              PYTEST="docker run greatexpectations/test:${DOCKER_TAG} "
              # We do NOT randomize the spark tests.
              # Spark only allows 1 context per JVM process, so we share it between tests.
              # Currently these tests order dependent.
              PYTEST+="pytest --spark --ignore=tests/cli --ignore=tests/integration/usage_statistics"
              echo ${PYTEST}
              eval ${PYTEST}
            displayName: 'Pytest Spark'

      - job: test_mysql
        dependsOn: [ make_suffix, build_push ]
        condition: succeeded()
        timeoutInMinutes: 120
        strategy:
          matrix:
            Python38:
              python.version: '3.8'
            Python39:
              python.version: '3.9'
            Python310:
              python.version: '3.10'
        services:
          mysql: mysql
        variables:
          IMAGE_SUFFIX: $[ dependencies.make_suffix.outputs['suffix.IMAGE_SUFFIX'] ]
        steps:
          - bash: |
              printf "Waiting for MySQL database to accept connections: "
              until mysql --host=localhost --protocol=TCP --port=3306 --user=root --password='' --execute "SHOW DATABASES"; do
                printf '.'
                sleep 1;
              done;
              printf "\n"
            displayName: 'Wait for MySQL'
          - bash: |
              mkdir script
              echo "SET GLOBAL sql_mode=(SELECT REPLACE(@@sql_mode,'ONLY_FULL_GROUP_BY',''));" > script/mysql_setup.sql
              DOCKER_TAG="$(python.version)_$(IMAGE_SUFFIX)"
              PYTEST="docker run -e AWS_ACCESS_KEY_ID=$(AWS_ACCESS_KEY_ID) "
              PYTEST+="-e AWS_SECRET_ACCESS_KEY=$(AWS_SECRET_ACCESS_KEY) -e AWS_DEFAULT_REGION=$(AWS_DEFAULT_REGION) "
              PYTEST+="--network=host --mount type=bind,source=${PWD}/script,target=/script "
              PYTEST+="greatexpectations/test:${DOCKER_TAG} /bin/bash -c \""
              PYTEST+="mysql --host=localhost --protocol=TCP --port=3306 --user=root --password='' --reconnect < /script/mysql_setup.sql && "
              # The last \" closes the quote from bash -c \".
              PYTEST+="pytest --mysql --random-order tests/test_definitions tests/expectations\""
              echo ${PYTEST}
              eval ${PYTEST}
            displayName: 'Pytest MySQL'
            env:
              AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
              AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

      - job: test_trino
        dependsOn: [make_suffix, build_push]
        condition: succeeded()
        timeoutInMinutes: 120
        strategy:
          matrix:
            Python38:
              python.version: '3.8'
            Python39:
              python.version: '3.9'
            Python310:
              python.version: '3.10'
        services:
          trino: trino
        variables:
          IMAGE_SUFFIX: $[ dependencies.make_suffix.outputs['suffix.IMAGE_SUFFIX'] ]
        steps:
          - bash: |
              printf 'Waiting for Trino database to accept connections'
              sleep 30
            displayName: 'Wait for Trino'
          - bash: |
              DOCKER_TAG="$(python.version)_$(IMAGE_SUFFIX)"
              PYTEST="docker run -e AWS_ACCESS_KEY_ID=$(AWS_ACCESS_KEY_ID) "
              PYTEST+="-e AWS_SECRET_ACCESS_KEY=$(AWS_SECRET_ACCESS_KEY) -e AWS_DEFAULT_REGION=$(AWS_DEFAULT_REGION) "
              PYTEST+="--network=host greatexpectations/test:${DOCKER_TAG} "
              PYTEST+="pytest --trino --random-order tests/test_definitions tests/expectations"
              echo ${PYTEST}
              eval ${PYTEST}
            displayName: 'Pytest Trino'
            env:
              AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
              AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
              AWS_DEFAULT_REGION: $(AWS_DEFAULT_REGION)

      - job: test_cli
        dependsOn: [make_suffix, build_push]
        condition: succeeded()
        strategy:
          matrix:
            Python38:
              python.version: '3.8'
            Python39:
              python.version: '3.9'
            Python310:
              python.version: '3.10'
            Python311:
              python.version: '3.11'
        services:
          postgres: postgres
        variables:
          IMAGE_SUFFIX: $[ dependencies.make_suffix.outputs['suffix.IMAGE_SUFFIX'] ]
        steps:
          - bash: |
              DOCKER_TAG="$(python.version)_$(IMAGE_SUFFIX)"
              PYTEST="docker run --network=host greatexpectations/test:${DOCKER_TAG} "
              PYTEST+="pytest --postgresql --spark --aws-integration tests/cli"
              echo ${PYTEST}
              eval ${PYTEST}
            displayName: 'Pytest cli'

      - job: test_usage_stats
        dependsOn: [make_suffix, build_push]
        condition: succeeded()
        strategy:
          matrix:
            Python38:
              python.version: '3.8'
            Python39:
              python.version: '3.9'
            Python310:
              python.version: '3.10'
        variables:
          IMAGE_SUFFIX: $[ dependencies.make_suffix.outputs['suffix.IMAGE_SUFFIX'] ]
        steps:
          - bash: |
              DOCKER_TAG="$(python.version)_$(IMAGE_SUFFIX)"
              PYTEST="docker run --network=host greatexpectations/test:${DOCKER_TAG} "
              PYTEST+="pytest --no-sqlalchemy --aws-integration --random-order tests/integration/usage_statistics"
              echo ${PYTEST}
              eval ${PYTEST}
            displayName: 'Pytest Usage Stats'

      - job: cloud_integration_tests
        dependsOn: [make_suffix, build_push]
        condition: succeeded()
        strategy:
          matrix:
            Python38:
              python.version: '3.8'
            Python39:
              python.version: '3.9'
            Python310:
              python.version: '3.10'
        variables:
          IMAGE_SUFFIX: $[ dependencies.make_suffix.outputs['suffix.IMAGE_SUFFIX'] ]
        steps:
          - bash: |
              DOCKER_TAG="$(python.version)_$(IMAGE_SUFFIX)"
              # We set both GX_* and GE_* in the docker container until we are able to update the config in our
              # cloud CI account.
              PYTEST="docker run -e GX_CLOUD_ORGANIZATION_ID=${GX_CLOUD_ORGANIZATION_ID} "
              PYTEST+="-e GX_CLOUD_ACCESS_TOKEN=${GX_CLOUD_ACCESS_TOKEN} -e GX_CLOUD_BASE_URL=${GX_CLOUD_BASE_URL} "
              PYTEST+="-e GE_CLOUD_ORGANIZATION_ID=${GX_CLOUD_ORGANIZATION_ID} "
              PYTEST+="-e GE_CLOUD_ACCESS_TOKEN=${GX_CLOUD_ACCESS_TOKEN} -e GE_CLOUD_BASE_URL=${GX_CLOUD_BASE_URL} "
              PYTEST+="greatexpectations/test:${DOCKER_TAG} "
              PYTEST+="pytest --random-order --cloud -m \"cloud and e2e\""
              echo ${PYTEST}
              eval ${PYTEST}
            env:
              GX_CLOUD_BASE_URL: $(GE_CLOUD_BASE_URL)
              GX_CLOUD_ACCESS_TOKEN: $(GE_CLOUD_ACCESS_TOKEN)
              GX_CLOUD_ORGANIZATION_ID: $(GE_CLOUD_ORGANIZATION_ID)
            displayName: 'Cloud integration Tests'


  - stage: import_ge
    dependsOn: [lint]
    pool:
      vmImage: 'ubuntu-latest'
    jobs:
      - job: import_ge
        condition: or(eq(variables.isScheduled, true), eq(variables.isReleasePrep, true), eq(variables.isRelease, true), eq(variables.isManual, true))

        strategy:
         matrix:
           Python38:
             python.version: '3.8'
           Python39:
             python.version: '3.9'
           Python310:
             python.version: '3.10'
           Python311:
             python.version: '3.11'

        steps:
         - task: UsePythonVersion@0
           inputs:
             versionSpec: '$(python.version)'
           displayName: 'Use Python $(python.version)'

         - bash: python -m pip install --upgrade pip
           displayName: 'Update pip'

         - script: |
             pip install  .
           displayName: 'Install GX and required dependencies (i.e. not sqlalchemy)'

         - script: |
             python -c "import great_expectations as gx; print('Successfully imported GX Version:', gx.__version__)"
           displayName: 'Import Great Expectations'

  - stage: db_integration
    pool:
      vmImage: 'ubuntu-latest'
    dependsOn: [lint, import_ge]
    jobs:
      - job: mssql
        condition: or(eq(variables.isScheduled, true), eq(variables.isReleasePrep, true), eq(variables.isRelease, true), eq(variables.isManual, true))
        timeoutInMinutes: 120

        services:
          mssql: mssql

        variables:
          python.version: '3.8'

        steps:
          - task: UsePythonVersion@0
            inputs:
              versionSpec: '$(python.version)'
            displayName: 'Use Python $(python.version)'

          - bash: python -m pip install --upgrade pip
            displayName: 'Update pip'

          - script: |
              sqlcmd -U sa -P "ReallyStrongPwd1234%^&*" -Q "CREATE DATABASE test_ci;" -o create_db_output.txt

          - script: |
              pip install --constraint constraints-dev.txt ".[test, mssql]" pytest-azurepipelines
            displayName: 'Install dependencies'

          - script: |
              pytest tests/test_definitions tests/expectations \
                --mssql \
                --napoleon-docstrings \
                --junitxml=junit/test-results.xml \
                --cov=. \
                --cov-report=xml \
                --cov-report=html
            displayName: 'Pytest mssql'

  # 2023-07-18 - Chetan - these are being disabled due to a test failure introduced by the provider
  # GitHub issue requiring resolution: https://github.com/astronomer/airflow-provider-great-expectations/issues/114

  # - stage: airflow_provider
  #   dependsOn: [lint, import_ge]
  #   pool:
  #     vmImage: 'ubuntu-latest'

  #   jobs:
  #     - job: run_airflow_provider_tests
  #       condition: or(eq(variables.isScheduled, true), eq(variables.isReleasePrep, true), eq(variables.isRelease, true), eq(variables.isManual, true))

  #       strategy:
  #         matrix:
  #           Python38:
  #             python.version: '3.8'
  #           Python39:
  #             python.version: '3.9'
  #           Python310:
  #             python.version: '3.10'
  #           Python311:
  #             python.version: '3.11'

  #       steps:
  #         - task: UsePythonVersion@0
  #           inputs:
  #             versionSpec: '$(python.version)'
  #           displayName: 'Use Python $(python.version)'

  #         - script: ./ci/checks/run_gx_airflow_operator_tests.sh
  #           name: RunAirflowProviderTests

  - stage: test_build_docs
    dependsOn: [lint, import_ge]
    pool:
      vmImage: 'ubuntu-latest'

    jobs:
      - job: test_build_docs
        condition: or(eq(variables.isScheduled, true), eq(variables.isReleasePrep, true), eq(variables.isRelease, true), eq(variables.isManual, true))

        steps:
          - task: UsePythonVersion@0
            inputs:
              versionSpec: '3.8'
            displayName: 'Use Python 3.8'

          - script: cd docs/docusaurus && yarn install && bash ../build_docs
            name: TestBuildDocs
            env:
              NODE_OPTIONS: --max_old_space_size=4096

  - stage: deploy
    condition: and(succeeded(), eq(variables.isRelease, true))
    pool:
      vmImage: 'ubuntu-latest'
    dependsOn: [import_ge, lint, db_integration, docker_tests]
    jobs:
      - job: deploy
        variables:
          python.version: '3.8'

        steps:
          - task: UsePythonVersion@0
            inputs:
              versionSpec: '$(python.version)'
            displayName: 'Use Python $(python.version)'

          - bash: python -m pip install --upgrade pip
            displayName: 'Update pip'

          - script: |
              pip install twine wheel
              git config --global user.email "team@greatexpectations.io"
              git config --global user.name "Great Expectations"
            displayName: 'Prepare packaging'

          # Build the python distribution from source
          - script: |
              python setup.py sdist
              python setup.py bdist_wheel
            displayName: 'Build distribution'

          - task: TwineAuthenticate@1
            inputs:
              pythonUploadServiceConnection: pypi

          # Use command line script to 'twine upload', use -r to pass the repository name and --config-file to pass the environment variable set by the authenticate task.
          - script: |
              python -m twine upload -r great-expectations --config-file $(PYPIRC_PATH) dist/*
            displayName: 'Upload'
