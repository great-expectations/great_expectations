import TabItem from '@theme/TabItem';
import Tabs from '@theme/Tabs';

import PrereqPythonInstall from '../../../../_core_components/prerequisites/_python_installation.md'
import PrereqGxInstall from '../../../../_core_components/prerequisites/_gx_installation_with_s3_dependencies.md'
import PrereqDataContext from '../../../../_core_components/prerequisites/_preconfigured_data_context.md'

### Prerequisites
- <PrereqPythonInstall/>
- <PrereqGxInstall/>
  - Optional. To create a Spark Filesystem Data Source you will also need to [install the Spark Python dependencies](/core/set_up_a_gx_environment/install_additional_dependencies.md?dependencies=spark).
- <PrereqDataContext/>
- Access to data files on a S3 bucket.

### Procedure

<Tabs 
   queryString="procedure"
   defaultValue="instructions"
   values={[
      {value: 'instructions', label: 'Instructions'},
      {value: 'sample_code', label: 'Sample code'}
   ]}
>

<TabItem value="instructions" label="Instructions">

1. Define the Data Source's parameters.

   The following information is required when you create an Amazon S3 Data Source:

   - `name`: A descriptive name used to reference the Data Source.  This should be unique within the Data Context.
   - `bucket_name`: The Amazon S3 bucket name.
   - `boto3_options`: Optional. Additional options for the Data Source. In the following examples, the default values are used.

   Replace the variable values with your own and run the following Python code to define `data_source_name`, `bucket_name` and `boto3_options`:

   ```python title="Python" name="docs/docusaurus/docs/core/connect_to_data/filesystem_data/_create_a_data_source/_s3/_spark.py - define Data Source parameters"
   ```

   :::info Additional options for `boto3_options`

   The parameter `boto3_options` allows you to pass the following information:

   - `region_name`: Your AWS region name.
   - `endpoint_url`: specifies an S3 endpoint.  You can provide an environment variable reference such as `"${S3_ENDPOINT}"` to securely include this in your code.  The string `"${S3_ENDPOINT}"` will be replaced with the value of the environment variable `S3_ENDPOINT`.
   
   For more information on secure storage and retrieval of credentials in GX see [Configure credentials](/core/connect_to_data/sql_data/sql_data.md#configure-credentials).

   :::

2. Add a S3 Filesystem Data Source to your Data Context.

   GX can leverage either pandas or Spark as the backend for your S3 Filesystem Data Source.  To create your Data Source, execute one of the following sets of code:
 
   <Tabs queryString="data_source_type" groupId="data_source_type" defaultValue='pandas_filesystem'>

   <TabItem value="pandas_filesystem" label="pandas">

   ```python title="Python" name="docs/docusaurus/docs/core/connect_to_data/filesystem_data/_create_a_data_source/_s3/_pandas.py - add Data Source"
   ```

   </TabItem>

   <TabItem value="spark" label="Spark">

   ```python title="Python" name="docs/docusaurus/docs/core/connect_to_data/filesystem_data/_create_a_data_source/_s3/_spark.py - add Data Source"
   ```

   </TabItem>

   </Tabs>

</TabItem>

<TabItem value="sample_code" label="Sample code">

   Choose from the following to see the full example code for a S3 Filesystem Data Source, using either pandas or Spark to read the data files:

   <Tabs queryString="data_source_type" groupId="data_source_type" defaultValue='pandas_filesystem'>

   <TabItem value="pandas_filesystem" label="pandas example">

   ```python title="Python" name="docs/docusaurus/docs/core/connect_to_data/filesystem_data/_create_a_data_source/_s3/_pandas.py - full example"
   ```

   </TabItem>

   <TabItem value="spark" label="Spark example">

   ```python title="Python" name="docs/docusaurus/docs/core/connect_to_data/filesystem_data/_create_a_data_source/_s3/_spark.py - full example"
   ```

   </TabItem>

   </Tabs>

</TabItem>

</Tabs>

