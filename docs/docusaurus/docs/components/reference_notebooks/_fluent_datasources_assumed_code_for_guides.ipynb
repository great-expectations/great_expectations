{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e4d71b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install great_expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489db6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! great_expectations --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98668639",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Filesystem data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6ad1ce",
   "metadata": {},
   "source": [
    "\n",
    "## How to connect to one or more files using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dependencies\n",
    "- An installation of GX\n",
    "- Source data (csv, excel, etc) in a local filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f59174",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import GX and instantiate a Data Context\n",
    "import great_expectations as gx\n",
    "\n",
    "context = gx.get_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f57936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Connect to the folder containing your Data\n",
    "path_to_folder_containing_csv_files = (\n",
    "    \"https://raw.githubusercontent.com/great_expectations/\"\n",
    ")\n",
    "datasource_name = \"MyNewDatasource\"\n",
    "datasource = context.datasources.add_pandas_filesystem(\n",
    "    name=datasource_name, base_path=path_to_folder_containing_csv_files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22c89dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add specific files to the Datasource as individual Data Assets\n",
    "csv_file_name_as_regex = r\"taxi_data\\.csv\"\n",
    "data_asset = datasource.add_csv_asset(\n",
    "    asset_name=\"MyTaxiDataAsset\", batching_regex=csv_file_name_as_regex\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a306eb",
   "metadata": {},
   "source": [
    "\n",
    "## How to connect to one or more files using Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dependencies\n",
    "- An installation of GX\n",
    "- Source data (csv, excel, etc) in a local filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4da09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import GX and instantiate a Data Context\n",
    "import great_expectations as gx\n",
    "\n",
    "context = gx.get_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2007b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Connect to the folder containing your Data\n",
    "path_to_folder_containing_csv_files = (\n",
    "    \"https://raw.githubusercontent.com/great_expectations/\"\n",
    ")\n",
    "datasource_name = \"MyNewDatasource\"\n",
    "datasource = context.datasources.add_spark_filesystem(\n",
    "    name=datasource_name, base_path=path_to_folder_containing_csv_files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca9ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add specific files as individual Data Assets\n",
    "csv_file_name_as_regex = r\"taxi_data\\.csv\"\n",
    "data_asset = datasource.add_csv_asset(\n",
    "    asset_name=\"MyTaxiDataAsset\", batching_regex=csv_file_name_as_regex\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65df4a09",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# In-memory data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0608db",
   "metadata": {},
   "source": [
    "\n",
    "## How to connect to in-memory data using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dependencies\n",
    "- An installation of GX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07882511",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import GX and instantiate a Data Context\n",
    "import great_expectations as gx\n",
    "\n",
    "context = gx.get_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0b7626",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sample data for our example:\n",
    "import Pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a46ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read your dataframe into a Datasource\n",
    "datasource_name = \"MyNewDatasource\"\n",
    "datasource = context.datasources.add_pandas_dataframe(name=datasource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66b6bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read the Dataframe as a Data Asset\n",
    "data_asset = datasource.read_dataframe(\n",
    "    dataframe=df, batch_identifiers=[\"default_identifier_name\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587a1804",
   "metadata": {},
   "source": [
    "\n",
    "## How to connect to in-memory data using Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dependencies\n",
    "- An installation of GX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c940cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import GX and instantiate a Data Context\n",
    "import great_expectations as gx\n",
    "\n",
    "context = gx.get_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79b5d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sample data for our example:\n",
    "df = [\n",
    "    {\"a\": 1, \"b\": 2, \"c\": 3},\n",
    "    {\"a\": 4, \"b\": 5, \"c\": 6},\n",
    "    {\"a\": 7, \"b\": 8, \"c\": 9},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb4b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read your dataframe into a Datasource\n",
    "datasource_name = \"MyNewDatasource\"\n",
    "datasource = context.datasources.add_spark_dataframe(name=datasource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228820e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read the Dataframe as a Data Asset\n",
    "data_asset = datasource.read_dataframe(\n",
    "    dataframe=df, batch_identifiers=[\"default_identifier_name\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f557be8b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Cloud data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81542bb7",
   "metadata": {},
   "source": [
    "\n",
    "## How to connect to data on Azure Blob Storage using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prerequisites\n",
    "- An installation of GX with ABS dependencies\n",
    "- Previously initialized a Data Context\n",
    "- ABS credentials configured\n",
    "- Source data (csv, excel, etc) in ABS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e195c2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there any dependencies for this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd92fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import GX and instantiate a Data Context\n",
    "import great_expectations as gx\n",
    "\n",
    "context = gx.get_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec8ca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Datasource\n",
    "datasource_name = \"MyDatasource\"\n",
    "# TODO: Any other required parameters? Path? No idea if htis is right.\n",
    "datasource = context.datasources.add_pandas_abs(\n",
    "    name=datasource_name, azure_options={\"account_url\": \"\", \"credentials\": \"\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da9b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add specific files to the Datasource as individual Data Assets\n",
    "csv_file_name_as_regex = r\"taxi_data\\.csv\"\n",
    "data_asset = datasource.add_csv_asset(\n",
    "    asset_name=\"MyTaxiDataAsset\", batching_regex=csv_file_name_as_regex\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c00b09",
   "metadata": {},
   "source": [
    "\n",
    "## How to connect to data on Azure Blob Storage using Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prerequisites\n",
    "- An installation of GX with ABS dependencies\n",
    "- Previously initialized a Data Context\n",
    "- ABS credentials configured\n",
    "- Source data (csv, excel, etc) in a ABS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8bb996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there any dependencies for this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442ce192",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import GX and instantiate a Data Context\n",
    "import great_expectations as gx\n",
    "\n",
    "context = gx.get_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0363391",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Datasource\n",
    "datasource_name = \"MyDatasource\"\n",
    "# TODO: Any other required parameters? Path? No idea if htis is right.\n",
    "datasource = context.datasources.add_spark_abs(\n",
    "    name=datasource_name, azure_options={\"account_url\": \"\", \"credentials\": \"\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2645c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add specific files to the Datasource as individual Data Assets\n",
    "csv_file_name_as_regex = r\"taxi_data\\.csv\"\n",
    "data_asset = datasource.add_csv_asset(\n",
    "    asset_name=\"MyTaxiDataAsset\", batching_regex=csv_file_name_as_regex\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c78f83",
   "metadata": {},
   "source": [
    "\n",
    "## How to connect to data on S3 using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prerequisites\n",
    "- An installation of GX with AWS S3 dependencies\n",
    "- Previously initialized a Data Context\n",
    "- AWS credentials configured\n",
    "- Source data (csv, excel, etc) in a S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4233a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install GX with S3 dependencies\n",
    "! pip install great_expectations[s3] # Installs Boto3 dependency\n",
    "! great_expectations --version\n",
    "\n",
    "# Configure credentials\n",
    "## Prerequisite: Install the AWS CLI\n",
    "### Confirm AWS CLI is installed; this will be used to configure AWS credentials.\n",
    "! aws --version\n",
    "### Confirm AWS credentials are configured properly\n",
    "! aws sts get-caller-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3363b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import GX and instantiate a Data Context\n",
    "import great_expectations as gx\n",
    "\n",
    "context = gx.get_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1fc5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Datasource\n",
    "datasource_name = \"MyS3Datasource\"\n",
    "bucket_path = \"YourBucketPath\"\n",
    "datasource = context.datasources.add_pandas_s3(name=datasource_name, bucket=bucket_path)\n",
    "### Optional\n",
    "boto3_options = {\n",
    "    \"endpoint_url\": \"${S3_ENDPOINT}\",  # Uses the S3_ENDPOINT environment variable to determine which endpoint to use.\n",
    "    \"region_name\": \"<your_aws_region_name>\",\n",
    "}\n",
    "datasource = context.datasource.add_pandas_s3(\n",
    "    name=datasource_name, bucket=bucket_path, boto3_options=boto3_options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab2ae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add specific files to the Datasource as individual Data Assets\n",
    "csv_file_name_as_regex = r\"taxi_data\\.csv\"\n",
    "data_asset = datasource.add_csv_asset(\n",
    "    asset_name=\"MyTaxiDataAsset\", batching_regex=csv_file_name_as_regex\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357f4d5b",
   "metadata": {},
   "source": [
    "\n",
    "## How to connect to data on S3 using Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prerequisites\n",
    "- An installation of GX with AWS S3 dependencies\n",
    "- Previously initialized a Data Context\n",
    "- AWS credentials configured\n",
    "- Source data (csv, excel, etc) in a S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a0820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install GX with S3 dependencies\n",
    "! pip install great_expectations[s3] # Installs Boto3 dependency\n",
    "! great_expectations --version\n",
    "\n",
    "# Configure credentials\n",
    "## Prerequisite: Install the AWS CLI\n",
    "### Confirm AWS CLI is installed; this will be used to configure AWS credentials.\n",
    "! aws --version\n",
    "### Confirm AWS credentials are configured properly\n",
    "! aws sts get-caller-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e67c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import GX and instantiate a Data Context\n",
    "import great_expectations as gx\n",
    "\n",
    "context = gx.get_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dd8e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Datasource\n",
    "datasource_name = \"MyS3Datasource\"\n",
    "bucket_path = \"YourBucketPath\"\n",
    "datasource = context.datasources.add_spark_s3(name=datasource_name, bucket=bucket_path)\n",
    "### Optional  (Do we need this? Should it be taken care of when setting up credentials with the AWS CLI?)\n",
    "boto3_options = {\n",
    "    \"endpoint_url\": \"${S3_ENDPOINT}\",  # Uses the S3_ENDPOINT environment variable to determine which endpoint to use.\n",
    "    \"region_name\": \"<your_aws_region_name>\",\n",
    "}\n",
    "datasource = context.datasource.add_spark_s3(\n",
    "    name=datasource_name, bucket=bucket_path, boto3_options=boto3_options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7684ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add specific files to the Datasource as individual Data Assets\n",
    "csv_file_name_as_regex = r\"taxi_data\\.csv\"\n",
    "data_asset = datasource.add_csv_asset(\n",
    "    asset_name=\"MyTaxiDataAsset\", batching_regex=csv_file_name_as_regex\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c178ac55",
   "metadata": {},
   "source": [
    "\n",
    "## How to connect to data on GCS using Google BigQuery and Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prerequisites\n",
    "- An installation of GX with BigQuery dependencies\n",
    "- Previously initialized a Data Context\n",
    "- BigQuery credentials configured\n",
    "- Source data (csv, excel, etc) in a GCP bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761b5d7e",
   "metadata": {},
   "source": [
    "### Configure credentials\n",
    "Great Expectations provides two options for configuring your GCS credentials:\n",
    "- Use the `gcloud` command line tool and `GOOGLE_APPLICATION_CREDENTIALS` environment variable\n",
    "  - This is the default option and what was used throughout this guide\n",
    "- Passing a filepath in as the value of an optional `credentials` parameter when you create your GBQ Datasource\n",
    "  - This argument should contain a specific filepath that leads to your credentials `.json` file\n",
    "  - This method utilizes `google.oauth2.service_account.Credentials.from_service_account_file` under the hood\n",
    "- Passing a JSON string value to the optional `credentials` parameter when you create your GBQ Datasource\n",
    "  - This string should contain the actual JSON data from your credentials file.\n",
    "  - This method utilizes `google.oauth2.service_account.Credentials.from_service_account_info` under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e109a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import GX and instantiate a Data Context\n",
    "import great_expectations as gx\n",
    "\n",
    "context = gx.get_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb984d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Datasource\n",
    "datasource = context.datasources.add_pandas_gbq(name=\"MyGbqDatasource\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db023d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add GCS data to the Datasource as a Data Asset\n",
    "csv_file_name = \"taxi_data.csv\"\n",
    "data_asset = datasource.add_csv_asset(asset_name=\"MyTaxiDataAsset\", regex=csv_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b81631",
   "metadata": {},
   "source": [
    "\n",
    "## How to connect to data on GCS using Google BigQuery and Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prerequisites\n",
    "- An installation of GX with BigQuery dependencies\n",
    "- Previously initialized a Data Context\n",
    "- BigQuery credentials configured\n",
    "- Source data (csv, excel, etc) in a GCP bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9226533",
   "metadata": {},
   "source": [
    "### Configure credentials\n",
    "Great Expectations provides two options for configuring your GCS credentials:\n",
    "- Use the `gcloud` command line tool and `GOOGLE_APPLICATION_CREDENTIALS` environment variable\n",
    "  - This is the default option and what was used throughout this guide\n",
    "- Passing a filepath in as the value of an optional `credentials` parameter when you create your GBQ Datasource\n",
    "  - This argument should contain a specific filepath that leads to your credentials `.json` file\n",
    "  - This method utilizes `google.oauth2.service_account.Credentials.from_service_account_file` under the hood\n",
    "- Passing a JSON string value to the optional `credentials` parameter when you create your GBQ Datasource\n",
    "  - This string should contain the actual JSON data from your credentials file.\n",
    "  - This method utilizes `google.oauth2.service_account.Credentials.from_service_account_info` under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27e0bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import GX and instantiate a Data Context\n",
    "import great_expectations as gx\n",
    "\n",
    "context = gx.get_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb0716",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Datasource\n",
    "datasource = context.datasources.add_spark_gbq(name=\"MyGbqDatasource\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c5c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add GCS data to the Datasource as a Data Asset\n",
    "csv_file_name = \"taxi_data.csv\"\n",
    "data_asset = datasource.add_csv_asset(asset_name=\"MyTaxiDataAsset\", regex=csv_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d907e6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# SQL Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebec787f",
   "metadata": {},
   "source": [
    "\n",
    "## How to connect to a SQL table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prerequisites\n",
    "- GX installed with SQL Dependencies\n",
    "- Source data in a SQL Database\n",
    "- Previously initialized Data Context\n",
    "- SQL Credentials configured\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657ae130",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import GX and instantiate a Data Context\n",
    "import great_expectations as gx\n",
    "\n",
    "context = gx.get_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2de44c",
   "metadata": {},
   "source": [
    "You can use either environment variables or a key in `config_variables.yml` to safely store any passwords needed by your connection string.  After defining your password in one of those ways, you can reference it in your connection string like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7bbfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define your connection string\n",
    "connection_string = \"postgresql+psycopg2://username:${MY_PASSWORD}@localhost/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686294af",
   "metadata": {},
   "source": [
    "In the above example `MY_PASSWORD` would be the name of the environment variable or the key to the value in `config_variables.yml` that corresponds to your credentials.\n",
    "\n",
    "If you include a password as plain text in your connection string when you define your Datasource, GX will automatically strip it out, add it to `config_variables.yml` and substitute it with a variable as was shown above.\n",
    "\n",
    "For purposes of this guide's examples, we will store our connection string in the variable `sql_connection_string` with plain text credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbe637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define your connection string, example\n",
    "sql_connection_string = \"postgresql+psycopg2://username:my_password@localhost/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855104bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Connect to the SQL database\n",
    "datasource = context.data_sources.add_sql(\n",
    "    name=\"my_datasource\", connection_string=sql_connection_string\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20331498",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add a table to the Datasource as a Data Asset\n",
    "table_asset = datasource.add_table_asset(\n",
    "    name=\"my_asset\", table_name=\"yellow_tripdata_sample\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad06a5c",
   "metadata": {},
   "source": [
    "\n",
    "## How to connect to SQL data using a query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c804e27f84196a10c8828c723f798",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prerequisites\n",
    "- GX installed with SQL Dependencies (Reference below for specific Database requirements)\n",
    "- Source data in a SQL Database\n",
    "- Previously initialized Data Context\n",
    "- Credentials configured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18430c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import GX and instantiate a Data Context\n",
    "import great_expectations as gx\n",
    "\n",
    "context = gx.get_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe8106f",
   "metadata": {},
   "source": [
    "You can use either environment variables or a key in `config_variables.yml` to safely store any passwords needed by your connection string.  After defining your password in one of those ways, you can reference it in your connection string like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af4403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define your connection string\n",
    "connection_string = \"postgresql+psycopg2://username:${MY_PASSWORD}@localhost/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c9b8ae",
   "metadata": {},
   "source": [
    "In the above example `MY_PASSWORD` would be the name of the environment variable or the key to the value in `config_variables.yml` that corresponds to your credentials.\n",
    "\n",
    "If you include a password as plain text in your connection string when you define your Datasource, GX will automatically strip it out, add it to `config_variables.yml` and substitute it with a variable as was shown above.\n",
    "\n",
    "For purposes of this guide's examples, we will store our connection string in the variable `sql_connection_string` with plain text credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44868c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define your connection string, example\n",
    "sql_connection_string = \"postgresql+psycopg2://username:my_password@localhost/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddb6263",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Connect to the SQL database\n",
    "datasource = context.data_sources.add_sql(\n",
    "    name=\"my_datasource\", connection_string=sql_connection_string\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed6a0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add a table to the Datasource as a Data Asset\n",
    "query_asset = datasource.add_query_asset(\n",
    "    name=\"my_asset\", query=\"SELECT * from yellow_tripdata_sample\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8be9e5",
   "metadata": {},
   "source": [
    "### PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c669311",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install GX with psycopg2 and sqlalchemy dependencies\n",
    "!pip install great_expectations[postgresql]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de1cd56",
   "metadata": {},
   "source": [
    "### Sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f58218",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install great_expectations[sqlalchemy]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc879f9",
   "metadata": {},
   "source": [
    "### Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87d686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install great_expectations[athena]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368be3b3",
   "metadata": {},
   "source": [
    "### Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f80c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install great_expectations[redshift]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f40180",
   "metadata": {},
   "source": [
    "### BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a3d4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install great_expectations[bigquery]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf552d9",
   "metadata": {},
   "source": [
    "### MSSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0682f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install great_expectations[mssql]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c07caf3",
   "metadata": {},
   "source": [
    "### Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01ab07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install great_expectations[snowflake]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4408ae0",
   "metadata": {},
   "source": [
    "### Trino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d87f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install great_expectations[trino]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6b6802",
   "metadata": {},
   "source": [
    "# Data Assets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81432ecb",
   "metadata": {},
   "source": [
    "## How to organize multiple files as Batches in a non-SQL Data Asset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504fb2a444614c0babb325280ed9130a",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prerequisites\n",
    "- An installation of GX\n",
    "- Previously initialized a Data Context\n",
    "- Configured a Datasource for filesystem style source data (csv, excel, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46cc9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import GX and instantiate a Data Context\n",
    "import great_expectations as gx\n",
    "\n",
    "context = gx.get_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d223f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get your filesystem datasource\n",
    "name_of_my_existing_datasource = \"MyConfiguredDatasource\"\n",
    "datasource = context.data_sources.get(name_of_my_existing_datasource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fb8744",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add all files matching a regex to a Datasource as a single Data Asset grouped by year and month\n",
    "all_csv_files_as_month_year_regex = (\n",
    "    r\"yellow_tripdata_sample_(?P<year>\\d{4})-(?P<month>\\d{2}).csv\"\n",
    ")\n",
    "data_asset = datasource.add_csv_asset(\n",
    "    asset_name=\"MyTaxiDataAsset\", batching_regex=all_csv_files_as_month_year_regex\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5233ff",
   "metadata": {},
   "source": [
    "## How to organize a SQL Data Asset into multiple Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bbdb311c014d738909a11f9e486628",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prerequisites\n",
    "- An installation of GX with necessary dependencies\n",
    "- Credentials set up for the SQL source data\n",
    "- Previously initialized a Data Context\n",
    "- Configured a Datasource for a SQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b0e422",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add a splitter to the Datasource\n",
    "table_asset.add_year_and_month_splitter(column_name=\"pickup_datetime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4d8e61",
   "metadata": {},
   "source": [
    ":::tip Splitters and Batch Identifiers\n",
    "\n",
    "When requesting data from a table Data Asset you can use the command `table_asset.batch_parameters_template()` to see how to specify your Batch Request.  This will include the Batch Identifier keys that your splitter has added to your table Data Asset.\n",
    "\n",
    "::: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adb15f7",
   "metadata": {},
   "source": [
    "## How to request a Batch of Data from a configured Data Asset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef1e04a",
   "metadata": {},
   "source": [
    "## How to request multiple Batches of Data from a configured Data Asset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c72614",
   "metadata": {},
   "source": [
    "## How to configure a non-SQL Data Asset to provide a sampling of its full data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910350d6",
   "metadata": {},
   "source": [
    "## How to configure a SQL Data Asset to provide a sampling of its full data"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
