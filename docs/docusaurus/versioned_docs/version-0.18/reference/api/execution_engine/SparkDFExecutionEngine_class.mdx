---
title: SparkDFExecutionEngine
sidebar_label: SparkDFExecutionEngine
---


<section class="sphinx-api-doc" id="sparkdfexecutionengine">

<dl class="py class">
<dt class="sig sig-object py" id="great_expectations.execution_engine.SparkDFExecutionEngine">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">great_expectations.execution_engine.</span></span><span class="sig-name descname"><span class="pre">SparkDFExecutionEngine</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">persist</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spark_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">dict</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spark</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">pyspark.sql.session.SparkSession</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">force_reuse_spark_context</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#great_expectations.execution_engine.SparkDFExecutionEngine" title="Permalink to this definition">#</a>
</dt>
<dd>
<p>SparkDFExecutionEngine instantiates the ExecutionEngine API to support computations using Spark platform.</p>
<p>This class holds an attribute <cite>spark_df</cite> which is a spark.sql.DataFrame.</p>
<p>Constructor builds a SparkDFExecutionEngine, using provided configuration parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> – Positional arguments for configuring SparkDFExecutionEngine</p></li>
<li><p><strong>persist</strong> – If True (default), then creation of the Spark DataFrame is done outside this class</p></li>
<li><p><strong>spark_config</strong> – Dictionary of Spark configuration options. If there is an existing Spark context,
the spark_config will be used to update that context in environments that allow it. In local
environments the Spark context will be stopped and restarted with the new spark_config.</p></li>
<li><p><strong>spark</strong> – A PySpark Session used to set the SparkDFExecutionEngine being configured. Will override
spark_config if provided.</p></li>
<li><p><strong>force_reuse_spark_context</strong> – If True then utilize existing SparkSession if it exists and is active</p></li>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 1.0: </span>The force_reuse_spark_context attribute is no longer part of any Spark Datasource classes. The existing Spark context will be reused if possible. If a spark_config is passed that doesn’t match the existing config, the context will be stopped and restarted in local environments only.</p>
</div>
<li><p><strong>**kwargs</strong> – Keyword arguments for configuring SparkDFExecutionEngine</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="great_expectations.execution_engine.SparkDFExecutionEngine.get_compute_domain">
<span class="sig-name descname"><span class="pre">get_compute_domain</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">domain_kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">domain_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="/docs/0.18/reference/api/core/metric_domain_types/MetricDomainTypes_class" title="great_expectations.core.metric_domain_types.MetricDomainTypes"><span class="pre">great_expectations.core.metric_domain_types.MetricDomainTypes</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">accessor_keys</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">pyspark.sql.dataframe.DataFrame</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#great_expectations.execution_engine.SparkDFExecutionEngine.get_compute_domain" title="Permalink to this definition">#</a>
</dt>
<dd>
<p>Uses a DataFrame and Domain kwargs (which include a row condition and a condition parser) to obtain and/or query a Batch of data.</p>
<p>Returns in the format of a Spark DataFrame along with Domain arguments required for computing.  If the Domain         is a single column, this is added to ‘accessor Domain kwargs’ and used for later access.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>domain_kwargs</strong> (<em>dict</em>) – a dictionary consisting of the Domain kwargs specifying which data to obtain</p></li>
<li><p><strong>domain_type</strong> (<em>str</em><em> or </em><a class="reference internal" href="/docs/0.18/reference/api/core/metric_domain_types/MetricDomainTypes_class" title="great_expectations.core.metric_domain_types.MetricDomainTypes"><em>MetricDomainTypes</em></a>) – an Enum value indicating which metric Domain the user would like             to be using, or a corresponding string value representing it.  String types include “identity”, “column”,             “column_pair”, “table” and “other”.  Enum types include capitalized versions of these from the class             MetricDomainTypes.</p></li>
<li><p><strong>accessor_keys</strong> (<em>str iterable</em>) – keys that are part of the compute Domain but should be ignored when             describing the Domain and simply transferred with their associated values into accessor_domain_kwargs.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even">
<p><ul class="simple">
<li><p>a DataFrame (the data on which to compute)</p></li>
<li><p>a dictionary of compute_domain_kwargs, describing the DataFrame</p></li>
<li><p>a dictionary of accessor_domain_kwargs, describing any accessors needed to
identify the Domain within the compute domain</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd">
<p>A tuple including</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="great_expectations.execution_engine.SparkDFExecutionEngine.get_domain_records">
<span class="sig-name descname"><span class="pre">get_domain_records</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">domain_kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><span class="pre">pyspark.sql.dataframe.DataFrame</span></span></span><a class="headerlink" href="#great_expectations.execution_engine.SparkDFExecutionEngine.get_domain_records" title="Permalink to this definition">#</a>
</dt>
<dd>
<p>Uses the given Domain kwargs (which include row_condition, condition_parser, and ignore_row_if directives) to obtain and/or query a batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd">
<p><strong>domain_kwargs</strong> (<em>dict</em>) – </p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even">
<p>A DataFrame (the data on which to compute returned in the format of a Spark DataFrame)</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</section>